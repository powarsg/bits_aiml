{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDfvlbucnId"
      },
      "source": [
        "# Assignment Walkthrough: Electricity Consumption Prediction\n",
        "\n",
        "\n",
        "## Problem Statement\n",
        "An electricity utility company wants to predict the weekly electricity consumption ($y$) based on operating hours ($x$).\n",
        "\n",
        "**Model:** Linear Regression $\\hat{y} = w_0 + w_1 x$\n",
        "\n",
        "**Loss Function:** Squared Error $J(w_0, w_1) = \\frac{1}{2}\\sum_{i=1}^{5}(\\hat{y}_i - y_i)^2$\n",
        "\n",
        "**Dataset:**\n",
        "| Shop | Operating Hours (x) | Consumption (y) |\n",
        "| --- | --- | --- |\n",
        "| 1 | 1 | 3 |\n",
        "| 2 | 2 | 5 |\n",
        "| 3 | 4 | 9 |\n",
        "| 4 | 6 | 13 |\n",
        "| 5 | 8 | 17 |"
      ],
      "id": "QVDfvlbucnId"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG4G2D-jcnIe",
        "outputId": "e8fc4b15-a741-46af-e02f-abf4b97a54ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset X: [1 2 4 6 8]\n",
            "Dataset Y: [ 3  5  9 13 17]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset\n",
        "X = np.array([1, 2, 4, 6, 8])\n",
        "Y = np.array([3, 5, 9, 13, 17])\n",
        "N = len(X)\n",
        "\n",
        "print(\"Dataset X:\", X)\n",
        "print(\"Dataset Y:\", Y)"
      ],
      "id": "lG4G2D-jcnIe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1G3r9IkcnIf"
      },
      "source": [
        "## Part (a): Loss Function & Convexity\n",
        "\n",
        "### 1. Loss Function\n",
        "The squared error loss function is:\n",
        "$$ J(w_0, w_1) = \\frac{1}{2} \\sum_{i=1}^{5} (w_0 + w_1 x_i - y_i)^2 $$\n",
        "\n",
        "### 2. Hessian Matrix\n",
        "The Hessian matrix $H$ contains the second-order partial derivatives:\n",
        "$$ H = \\begin{bmatrix} \\frac{\\partial^2 J}{\\partial w_0^2} & \\frac{\\partial^2 J}{\\partial w_0 \\partial w_1} \\\\ \\frac{\\partial^2 J}{\\partial w_1 \\partial w_0} & \\frac{\\partial^2 J}{\\partial w_1^2} \\end{bmatrix} $$\n",
        "\n",
        "Where:\n",
        "- $\\frac{\\partial^2 J}{\\partial w_0^2} = N$\n",
        "- $\\frac{\\partial^2 J}{\\partial w_0 \\partial w_1} = \\sum x_i$\n",
        "- $\\frac{\\partial^2 J}{\\partial w_1^2} = \\sum x_i^2$"
      ],
      "id": "M1G3r9IkcnIf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlMSWR5ycnIf",
        "outputId": "6d04089b-4f3c-47f8-e30f-42eb5ef30a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hessian Matrix:\n",
            " [[  5  21]\n",
            " [ 21 121]]\n"
          ]
        }
      ],
      "source": [
        "def get_hessian(X):\n",
        "    h_00 = len(X)       # N\n",
        "    h_01 = np.sum(X)    # Sum of x\n",
        "    h_10 = np.sum(X)    # Sum of x\n",
        "    h_11 = np.sum(X**2) # Sum of x^2\n",
        "    return np.array([[h_00, h_01], [h_10, h_11]])\n",
        "\n",
        "H = get_hessian(X)\n",
        "print(\"Hessian Matrix:\\n\", H)"
      ],
      "id": "MlMSWR5ycnIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2A1quKzcnIf"
      },
      "source": [
        "### 3. Convexity Proof\n",
        "A function is strictly convex if its Hessian is Positive Definite. We can verify this by checking if all eigenvalues are positive."
      ],
      "id": "_2A1quKzcnIf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-rjqHc_cnIf",
        "outputId": "2301307a-5b3a-4948-afde-a0b3f91e93e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues: [  1.31531795 124.68468205]\n",
            "Conclusion: The Hessian is Positive Definite, so the Loss Function is Strictly Convex.\n"
          ]
        }
      ],
      "source": [
        "eigenvalues = np.linalg.eigvals(H)\n",
        "print(\"Eigenvalues:\", eigenvalues)\n",
        "\n",
        "if np.all(eigenvalues > 0):\n",
        "    print(\"Conclusion: The Hessian is Positive Definite, so the Loss Function is Strictly Convex.\")\n",
        "else:\n",
        "    print(\"Conclusion: The Loss Function is NOT Strictly Convex.\")"
      ],
      "id": "u-rjqHc_cnIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq29Vcd4cnIf"
      },
      "source": [
        "## Part (b): Gradient Calculation\n",
        "\n",
        "The gradient vector $\\nabla J$ is composed of the first derivatives:\n",
        "$$ \\nabla J = \\begin{bmatrix} \\frac{\\partial J}{\\partial w_0} \\\\ \\frac{\\partial J}{\\partial w_1} \\end{bmatrix} = \\begin{bmatrix} \\sum (\\hat{y}_i - y_i) \\\\ \\sum (\\hat{y}_i - y_i)x_i \\end{bmatrix} $$"
      ],
      "id": "Hq29Vcd4cnIf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWAS_7SVcnIf",
        "outputId": "51f8f432-ccba-48f0-9688-24f8c9967ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Gradient at w=[2. 2.]: [ 5. 21.]\n"
          ]
        }
      ],
      "source": [
        "def get_loss(w0, w1, X, Y):\n",
        "    y_pred = w0 + w1 * X\n",
        "    error = y_pred - Y\n",
        "    loss = 0.5 * np.sum(error**2)\n",
        "    return loss\n",
        "\n",
        "def get_gradient(w0, w1, X, Y):\n",
        "    y_pred = w0 + w1 * X\n",
        "    error = y_pred - Y\n",
        "    grad_w0 = np.sum(error)\n",
        "    grad_w1 = np.sum(error * X)\n",
        "    return np.array([grad_w0, grad_w1])\n",
        "\n",
        "w_initial = np.array([2.0, 2.0])\n",
        "grad_initial = get_gradient(w_initial[0], w_initial[1], X, Y)\n",
        "print(f\"Initial Gradient at w={w_initial}: {grad_initial}\")"
      ],
      "id": "iWAS_7SVcnIf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrWBPxIlcnIg"
      },
      "source": [
        "## Part (c): Gradient Descent Iterations\n",
        "\n",
        "We perform two iterations of Gradient Descent starting from $w^{(0)} = [2, 2]^T$.\n",
        "\n",
        "**Update Rule:** $w^{(t+1)} = w^{(t)} - \\eta \\nabla J(w^{(t)})$\n",
        "\n",
        "### Scenario 1: Constant Learning Rate ($\\eta = 0.05$)"
      ],
      "id": "wrWBPxIlcnIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc-DGX0ucnIg",
        "outputId": "55b3a57f-a4ac-4fad-edee-269c1f351529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Scenario 1: Constant LR ---\n",
            "Iter 0: w = [2. 2.], Loss = 2.5, Grad = [ 5. 21.]\n",
            "Iter 1: w = [1.75 0.95], Loss = 51.57000000000001, Grad = [ -18.3 -111.3]\n",
            "Iter 2: w = [2.665 6.515], Loss = 1398.103650000001\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Scenario 1: Constant LR ---\")\n",
        "w = w_initial.copy()\n",
        "eta = 0.05\n",
        "\n",
        "for i in range(1, 3):\n",
        "    grad = get_gradient(w[0], w[1], X, Y)\n",
        "    loss = get_loss(w[0], w[1], X, Y)\n",
        "    print(f\"Iter {i-1}: w = {w}, Loss = {loss}, Grad = {grad}\")\n",
        "\n",
        "    w = w - eta * grad\n",
        "\n",
        "print(f\"Iter 2: w = {w}, Loss = {get_loss(w[0], w[1], X, Y)}\")"
      ],
      "id": "qc-DGX0ucnIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejiMZqGncnIg"
      },
      "source": [
        "### Scenario 2: Decaying Learning Rate\n",
        "$\\eta_t = \\eta_0 e^{-kt}$, where $\\eta_0 = 0.1, k = 0.4$."
      ],
      "id": "ejiMZqGncnIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL0K63OwcnIg",
        "outputId": "46baacc0-bb6a-4ff7-abcb-a85c406a32aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Scenario 2: Decaying LR ---\n",
            "Iter 0: w = [2. 2.], Loss = 2.50, Grad = [ 5. 21.], eta_1 = 0.06703\n",
            "Iter 1: w = [1.66483998 0.5923279 ], Loss = 101.33, Grad = [ -26.23691415 -156.36668418], eta_2 = 0.04493\n",
            "Iter 2: w = [2.84374052 7.61833593], Loss = 2135.76\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Scenario 2: Decaying LR ---\")\n",
        "w = w_initial.copy()\n",
        "eta_0 = 0.1\n",
        "k = 0.4\n",
        "\n",
        "for i in range(1, 3):\n",
        "    # Calculate eta for the current step (t=1, then t=2)\n",
        "    eta_t = eta_0 * np.exp(-k * i)\n",
        "\n",
        "    grad = get_gradient(w[0], w[1], X, Y)\n",
        "    loss = get_loss(w[0], w[1], X, Y)\n",
        "    print(f\"Iter {i-1}: w = {w}, Loss = {loss:.2f}, Grad = {grad}, eta_{i} = {eta_t:.5f}\")\n",
        "\n",
        "    w = w - eta_t * grad\n",
        "\n",
        "print(f\"Iter 2: w = {w}, Loss = {get_loss(w[0], w[1], X, Y):.2f}\")"
      ],
      "id": "DL0K63OwcnIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0faIsACcnIg"
      },
      "source": [
        "## Part (d): Stability Analysis\n",
        "\n",
        "**Observation:** Both scenarios diverged (loss increased significantly).\n",
        "\n",
        "**Why?** The learning rates used (0.05 and ~0.067) are too large relative to the curvature of the loss function.\n",
        "The maximum stable learning rate is generally $\\frac{2}{\\lambda_{max}}$, where $\\lambda_{max}$ is the largest eigenvalue of the Hessian.\n",
        "\n",
        "Let's check the limit:"
      ],
      "id": "m0faIsACcnIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o31tDA6AcnIg",
        "outputId": "d963b75c-5f2e-4fa8-a70e-c636c0281011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Stable Learning Rate: 0.01604\n"
          ]
        }
      ],
      "source": [
        "max_stable_lr = 2 / np.max(eigenvalues)\n",
        "print(f\"Max Stable Learning Rate: {max_stable_lr:.5f}\")"
      ],
      "id": "o31tDA6AcnIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tQ_WY8EcnIg"
      },
      "source": [
        "Since our learning rates ($0.05$ and $0.067$) are much larger than $0.016$, the optimization overshoots and diverges.\n",
        "\n",
        "## Part (e): Line Search Methods\n",
        "\n",
        "We search for the optimal step size $\\eta$ along the steepest descent direction $d = -\\nabla J(w^{(0)})$.\n",
        "We minimize $\\phi(\\eta) = J(w^{(0)} + \\eta d)$."
      ],
      "id": "3tQ_WY8EcnIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA_9KfjIcnIg",
        "outputId": "90a4f058-3a7b-4770-d0bc-5246b8622b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Direction d: [ -5. -21.]\n",
            "phi(0) = 2.5\n"
          ]
        }
      ],
      "source": [
        "d = -grad_initial\n",
        "print(f\"Search Direction d: {d}\")\n",
        "\n",
        "def phi(eta):\n",
        "    w_new = w_initial + eta * d\n",
        "    return get_loss(w_new[0], w_new[1], X, Y)\n",
        "\n",
        "print(f\"phi(0) = {phi(0)}\")"
      ],
      "id": "vA_9KfjIcnIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMuXyR7LcnIg"
      },
      "source": [
        "### 1. Binary Search\n",
        "We compare $\\phi(m)$ and $\\phi(m + \\epsilon)$ to estimate the slope and reduce the interval $[a, b]$."
      ],
      "id": "HMuXyR7LcnIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emW2gj38cnIg",
        "outputId": "bf937af4-27aa-4bdf-ec39-e769fe365588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Interval: [0.0, 1.0]\n",
            "Iter 1: m=0.5, phi(m)=7006.50, phi(m+eps)=7035.01\n",
            "  -> Increasing function. Min is to the LEFT.\n",
            "  New Interval: [0.0, 0.5]\n",
            "Iter 2: m=0.25, phi(m)=1695.25, phi(m+eps)=1709.29\n",
            "  -> Increasing function. Min is to the LEFT.\n",
            "  New Interval: [0.0, 0.25]\n"
          ]
        }
      ],
      "source": [
        "a, b = 0.0, 1.0\n",
        "epsilon = 1e-3\n",
        "\n",
        "print(f\"Initial Interval: [{a}, {b}]\")\n",
        "\n",
        "for i in range(1, 3):\n",
        "    m = (a + b) / 2\n",
        "    val_m = phi(m)\n",
        "    val_eps = phi(m + epsilon)\n",
        "\n",
        "    print(f\"Iter {i}: m={m}, phi(m)={val_m:.2f}, phi(m+eps)={val_eps:.2f}\")\n",
        "\n",
        "    if val_m < val_eps:\n",
        "        print(\"  -> Increasing function. Min is to the LEFT.\")\n",
        "        b = m\n",
        "    else:\n",
        "        print(\"  -> Decreasing function. Min is to the RIGHT.\")\n",
        "        a = m\n",
        "    print(f\"  New Interval: [{a}, {b}]\")"
      ],
      "id": "emW2gj38cnIg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72qnCyEvcnIg"
      },
      "source": [
        "### 2. Golden-Section Search\n",
        "We use interior points at $1/4$ and $3/4$ of the current interval width."
      ],
      "id": "72qnCyEvcnIg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DBV6wQqcnIg",
        "outputId": "35ac0846-4b9d-48c2-a5f8-9a3056b84f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Interval: [0.0, 1.0]\n",
            "Iter 1: M1=0.25, M2=0.75\n",
            "  phi(M1)=1695.25, phi(M2)=15936.25\n",
            "  -> phi(M1) < phi(M2). Min is in [a, M2].\n",
            "  New Interval: [0.0, 0.75]\n",
            "Iter 2: M1=0.1875, M2=0.5625\n",
            "  phi(M1)=932.83, phi(M2)=8899.70\n",
            "  -> phi(M1) < phi(M2). Min is in [a, M2].\n",
            "  New Interval: [0.0, 0.5625]\n"
          ]
        }
      ],
      "source": [
        "a, b = 0.0, 1.0\n",
        "print(f\"Initial Interval: [{a}, {b}]\")\n",
        "\n",
        "for i in range(1, 3):\n",
        "    width = b - a\n",
        "    m1 = a + 0.25 * width\n",
        "    m2 = a + 0.75 * width\n",
        "\n",
        "    val_m1 = phi(m1)\n",
        "    val_m2 = phi(m2)\n",
        "\n",
        "    print(f\"Iter {i}: M1={m1}, M2={m2}\")\n",
        "    print(f\"  phi(M1)={val_m1:.2f}, phi(M2)={val_m2:.2f}\")\n",
        "\n",
        "    if val_m1 < val_m2:\n",
        "        print(\"  -> phi(M1) < phi(M2). Min is in [a, M2].\")\n",
        "        b = m2\n",
        "    else:\n",
        "        print(\"  -> phi(M1) >= phi(M2). Min is in [M1, b].\")\n",
        "        a = m1\n",
        "    print(f\"  New Interval: [{a}, {b}]\")"
      ],
      "id": "6DBV6wQqcnIg"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}