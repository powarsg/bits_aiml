I want to do CNN asiignment.

Instructions are mentioned in : CNN_Assignment_Instructions.pdf
template notebook file : CNN_assignment_template.ipynb

Technical Requirements 1 Framework Choice
Choose ONE framework (do NOT mix):
Option 1: Keras/TensorFlow
Option 2: PyTorch 2 Dataset

Requirements :
1. Cats vs Dogs (2 classes)
2. Minimum: 500 images PER CLASS (not total)
3. Total images = min 1000

2.2.Load your chosen dataset
- Submit ONLY the notebook file.
- Load data from public sources or URLs.

2.3 Data Exploration and Visualization
- TODO: Show sample images from each class
- TODO: Plot class distribution
- TODO: Display image statistics

2.4 Data Preprocessing
- TODO: Resize images to consistent size
- TODO: Normalize pixel values
- TODO: Split into train/test (90/10 or 85/15) (choose one)

2.5 Image Size: Typically 224 X 224 for transfer learning

3. Global Average Pooling (GAP) - MANDATORY BOTH models MUST use Global Average Pooling.

4 Assignment Components
4.1. Part 1: Custom CNN : Build a CNN using Keras or PyTorch with:
1. At least 2 Conv2D layers
2. Pooling layers (MaxPooling or AveragePooling)
3. Global Average Pooling (MANDATORY)
4. Output layer with Softmax activation

IMPORTANT: Global Average Pooling (GAP) is MANDATORY for both models.
DO NOT use Flatten + Dense layers in the final architecture.

What to Track
- Initial loss (first epoch)
- Final loss (last epoch)
- Training time
- All 4 metrics: accuracy, precision, recall, F1-score

4.2 Part 2: Transfer Learning Use a pre-trained model:
- Choose ONE: ResNet18, ResNet50, VGG16, or VGG19
- Freeze base layers (feature extractor)
- Add custom classification head
- Fine-tune on your dataset


Metric    Custom CNN  Transfer Learning
       Accuracy      0.771875           0.615625
      Precision      0.771773           0.629733
         Recall      0.771359           0.620039
       F1-Score      0.771498           0.609830
Training Time (s)    920.972648         368.317837
     Parameters 388673.000000      262401.000000

     Key Topics:
     1. Performance comparison with specific metrics
     2. Pre-training vs training from scratch impact
     3. GAP effect on performance/overfitting
     4. Computational cost comparison
     5. Transfer learning insights
     6. Convergence behavior differences

The custom CNN achieved higher performance across all evaluation metrics compared to the transfer learning model. Specifically, the custom CNN obtained an accuracy approximately 0.77, while the transfer learning model achieved around 0.62
Pre-training significantly impacts learning behavior. The transfer learning model benefited from pretrained ImageNet features, allowing faster convergence with fewer epochs. However, since the base layers were frozen and fine-tuning was limited, the model could not fully adapt to the target dataset. In contrast, the custom CNN learned features from scratch, requiring more epochs but achieving better task-specific performance.
Global Average Pooling (GAP) was effective in both models by reducing the number of parameters and mitigating overfitting. By replacing fully connected layers, GAP improved generalization while maintaining spatial feature aggregation.
From a computational perspective, the custom CNN required more training time due to a higher number of epochs, whereas the transfer learning model trained faster despite a heavier backbone. This demonstrates that training time depends not only on parameter count but also on architectural complexity.
Overall, transfer learning is advantageous when rapid convergence is required or data is limited, while a well-tuned custom CNN can outperform frozen pretrained models when sufficient training time is available.
